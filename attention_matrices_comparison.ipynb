{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------\n",
    "# Hyperparams\n",
    "# ----------\n",
    "batch_size = 1\n",
    "seq_len    = 2048    # Large \"context window\"\n",
    "d_model    = 32    # Embedding dimension\n",
    "intrinsic_rank = 128 # We'll artificially force Q, K, V to have rank <= 8\n",
    "proj_k     = seq_len // 4     # Linformer projection dimension\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Create Low-Rank Q, K, V\n",
    "#    We'll do Q = (Q_base) x (Q_proj) so that rank(Q) <= intrinsic_rank\n",
    "# ---------------------------------------------------------------------\n",
    "def make_low_rank_matrix(seq_len, d_model, intrinsic_rank):\n",
    "    \"\"\"\n",
    "    Create a matrix of shape (seq_len, d_model) with max rank = intrinsic_rank.\n",
    "    We'll create it by multiplying:\n",
    "       [seq_len x intrinsic_rank] x [intrinsic_rank x d_model]\n",
    "    \"\"\"\n",
    "    base  = torch.randn(seq_len, intrinsic_rank)\n",
    "    proj  = torch.randn(intrinsic_rank, d_model)\n",
    "    full  = base @ proj  # shape: (seq_len, d_model)\n",
    "    return full\n",
    "\n",
    "# For demonstration, keep it batch_size=1\n",
    "Q = make_low_rank_matrix(seq_len, d_model, intrinsic_rank).unsqueeze(0)  # (1, seq_len, d_model)\n",
    "K = make_low_rank_matrix(seq_len, d_model, intrinsic_rank).unsqueeze(0)  # (1, seq_len, d_model)\n",
    "V = make_low_rank_matrix(seq_len, d_model, intrinsic_rank).unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) Vanilla Attention (Full Softmax Attention)\n",
    "# ---------------------------------------------\n",
    "# QK^T has shape [batch, seq_len, seq_len]\n",
    "scores_vanilla = torch.bmm(Q, K.transpose(1,2)) / (d_model**0.5)\n",
    "attn_weights_vanilla = F.softmax(scores_vanilla, dim=-1)  # [1, seq_len, seq_len]\n",
    "output_vanilla = torch.bmm(attn_weights_vanilla, V)       # [1, seq_len, d_model]\n",
    "\n",
    "print(\"=== Vanilla Attention ===\")\n",
    "print(\"Attention matrix shape:\", attn_weights_vanilla.shape, \"=> (batch, seq_len, seq_len)\")\n",
    "print(\"Output shape:\", output_vanilla.shape, \"=> (batch, seq_len, d_model)\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) Demonstrate (Approx.) Low Rank via Singular Values\n",
    "#    We'll do SVD on a single attention matrix instance\n",
    "# --------------------------------------------------------\n",
    "# Extract the attention matrix for the single batch dimension\n",
    "A = attn_weights_vanilla[0]  # shape (seq_len, seq_len)\n",
    "# Torch has torch.linalg.svd or torch.svd. We'll use torch.linalg.svd:\n",
    "U, S, Vh = torch.linalg.svd(A)\n",
    "\n",
    "print(\"\\nSingular Values of the Vanilla Attention Matrix:\")\n",
    "print(S)\n",
    "\n",
    "# Print how many singular values are \"significant\"\n",
    "threshold = 1e-3\n",
    "significant_sv = (S > threshold).sum().item()\n",
    "print(f\"Number of singular values > {threshold}: {significant_sv} out of {seq_len}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4) Linformer Attention: Project K, V along sequence dimension\n",
    "# ----------------------------------------------------------------\n",
    "# E_K, E_V are trainable or fixed in practice; here, random for demo\n",
    "E_K = torch.randn(seq_len, proj_k)\n",
    "E_V = torch.randn(seq_len, proj_k)\n",
    "\n",
    "# Project K and V to shape: [batch, seq_len, proj_k] (conceptually)\n",
    "# But because we have them as (b, seq_len, d_model), we do an einsum:\n",
    "K_linformer = torch.einsum('bsd,sp->bpd', K, E_K)  # => (b, proj_k, d_model)\n",
    "V_linformer = torch.einsum('bsd,sp->bpd', V, E_V)  # => (b, proj_k, d_model)\n",
    "\n",
    "# Then compute Q x K_linformer^T => [b, seq_len, d_model] x [b, d_model, proj_k] => [b, seq_len, proj_k]\n",
    "scores_linformer = torch.bmm(Q, K_linformer.transpose(1,2)) / (d_model**0.5)\n",
    "attn_weights_linformer = F.softmax(scores_linformer, dim=-1)  # [b, seq_len, proj_k]\n",
    "output_linformer = torch.bmm(attn_weights_linformer, V_linformer)  # [b, seq_len, d_model]\n",
    "\n",
    "print(\"\\n=== Linformer Attention ===\")\n",
    "print(\"Projected K shape:\", K_linformer.shape, \"-> (batch, proj_k, d_model)\")\n",
    "print(\"Projected V shape:\", V_linformer.shape, \"-> (batch, proj_k, d_model)\")\n",
    "print(\"QK'^T shape:\", scores_linformer.shape, \"-> (batch, seq_len, proj_k)\")\n",
    "print(\"Linformer attention matrix shape:\", attn_weights_linformer.shape, \"-> (batch, seq_len, proj_k)\")\n",
    "print(\"Linformer final output shape:\", output_linformer.shape, \"-> (batch, seq_len, d_model)\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5) Demonstrate Reduced Memory / Compare Shapes\n",
    "# ----------------------------------------------\n",
    "print(\"\\nMemory (Full attn matrix): seq_len x seq_len =\", seq_len * seq_len)\n",
    "print(\"Memory (Linformer attn matrix): seq_len x proj_k =\", seq_len * proj_k)\n"
   ],
   "id": "e8585c37a7feda5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a860aad23e99056"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
