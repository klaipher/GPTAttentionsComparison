{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 10817521,
     "sourceType": "datasetVersion",
     "datasetId": 6716316
    }
   ],
   "dockerImageVersionId": 30919,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.725968Z",
     "iopub.execute_input": "2025-02-22T15:11:49.726331Z",
     "iopub.status.idle": "2025-02-22T15:11:49.732004Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.726302Z",
     "shell.execute_reply": "2025-02-22T15:11:49.730391Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.254085Z",
     "start_time": "2025-02-22T16:49:57.251004Z"
    }
   },
   "outputs": [],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "source": "# Attentions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Vanilla",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class VanillaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                 .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
    "                                                               is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = nn.functional.softmax(att, dim=-1)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y "
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.733557Z",
     "iopub.execute_input": "2025-02-22T15:11:49.734058Z",
     "iopub.status.idle": "2025-02-22T15:11:49.761872Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.734017Z",
     "shell.execute_reply": "2025-02-22T15:11:49.759956Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.285298Z",
     "start_time": "2025-02-22T16:49:57.280503Z"
    }
   },
   "outputs": [],
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "source": "## Nystrom",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class NystromAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Linformer self-attention mechanism with linear complexity.\n",
    "    Projects keys and values to a lower dimensional space for efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Default Linformer config\n",
    "        self.n_landmarks = config.attention_config.get('nystrom_landmarks', 32) if config.attention_config else 32\n",
    "        print(f'Nystrom with landmarks {self.n_landmarks}')\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "        # mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        hs = C // self.n_head\n",
    "\n",
    "        # calculate query, key, values for all heads in batch\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        \n",
    "        params = {'B': B, 'nh': self.n_head, 'T': T, 'hs': hs}\n",
    "        \n",
    "        # Project keys and values to lower dimensional space\n",
    "        q_landmarks = self.__get_landmark_representation(q, self.n_landmarks, **params)\n",
    "        k_landmarks = self.__get_landmark_representation(k, self.n_landmarks, **params)\n",
    "        \n",
    "        # Compute the attention matrix\n",
    "        L = F.softmax(q @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1)\n",
    "        P = self.__iterative_inv(F.softmax(q_landmarks @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1))\n",
    "\n",
    "        N_prod = (q_landmarks @ k.transpose(-1, -2))\n",
    "        # print(N_prod.shape)\n",
    "        # print(self.bias.shape)\n",
    "        # print(q_landmarks.shape)\n",
    "        N_masked = N_prod.masked_fill(self.bias[:, :, :self.n_landmarks, :T] == 0, float('-inf'))\n",
    "        N = F.softmax(N_masked / math.sqrt(hs), dim=-1)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        att = L @ P @ N\n",
    "\n",
    "        # Apply attention to values and reshape\n",
    "        y = att @ v  # (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        return self.c_proj(y)\n",
    "    \n",
    "    def __get_landmark_representation(self, tensor, num_landmarks, B, nh, T, hs):\n",
    "        tensor_reshaped = tensor.reshape(-1, nh, num_landmarks, T // num_landmarks, hs) # (B, nh, T, hs)\n",
    "        tensor_landmarks = tensor_reshaped.mean(dim=-2)\n",
    "        return tensor_landmarks\n",
    "\n",
    "    def __iterative_inv(self, mat, n_iter=6):\n",
    "        I = torch.eye(mat.size(-1), device=mat.device)\n",
    "        K = mat\n",
    "\n",
    "        # The entries of K are positive and ||K||_{\\infty} = 1 due to softmax\n",
    "        V = 1 / torch.max(torch.sum(K, dim=-2), dim = -1).values[:, :, None, None] * K.transpose(-1, -2)\n",
    "\n",
    "        for _ in range(n_iter):\n",
    "            KV = torch.matmul(K, V)\n",
    "            V = torch.matmul(0.25 * V, 13 * I - torch.matmul(KV, 15 * I - torch.matmul(KV, 7 * I - KV)))\n",
    "        return V"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:18:45.393591Z",
     "iopub.execute_input": "2025-02-22T15:18:45.393946Z",
     "iopub.status.idle": "2025-02-22T15:18:45.409608Z",
     "shell.execute_reply.started": "2025-02-22T15:18:45.393917Z",
     "shell.execute_reply": "2025-02-22T15:18:45.408276Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.298646Z",
     "start_time": "2025-02-22T16:49:57.292128Z"
    }
   },
   "outputs": [],
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "source": "## Linformer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class LinformerAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "\n",
    "        # default: k = block_size // 4, or from config\n",
    "        self.linformer_k = config.attention_config.get('linformer_k', config.block_size//4) \\\n",
    "                           if config.attention_config else config.block_size//4\n",
    "\n",
    "        # Q, K, V projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=True)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=True)\n",
    "\n",
    "        # Initialize E and F with mean of K and V projections\n",
    "        self.E = self.initialize_projection_matrix(config.block_size, self.linformer_k)\n",
    "        self.F = self.initialize_projection_matrix(config.block_size, self.linformer_k)\n",
    "\n",
    "        # naive causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def initialize_projection_matrix(self, block_size, linformer_k):\n",
    "        # Initialize a tensor that averages over the input tokens\n",
    "        init_matrix = torch.ones(self.n_head, linformer_k, block_size) / block_size\n",
    "        return nn.Parameter(init_matrix)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # slice E, F to current T => shape (n_head, linformer_k, T)\n",
    "        E = self.E[:, :, :T]\n",
    "        F = self.F[:, :, :T]\n",
    "\n",
    "        # project K, V\n",
    "        k_projected = torch.einsum('hkt,bhtd->bhkd', E, k)  # (B, n_head, k_lin, head_size)\n",
    "        v_projected = torch.einsum('hkt,bhtd->bhkd', F, v)  # (B, n_head, k_lin, head_size)\n",
    "\n",
    "        # attention\n",
    "        att = torch.matmul(q, k_projected.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
    "        # 'att' is (B, n_head, T, linformer_k)\n",
    "\n",
    "        # naive masking: we slice the standard T x T mask to T x k\n",
    "        # This is not truly correct for a \"causal Linformer,\" but it at least\n",
    "        # prevents the largest mismatch where Linformer sees all tokens.\n",
    "        causal_mask = self.bias[:, :, :T, :k_projected.size(2)]  # shape (1,1,T,k_lin)\n",
    "        att = att.masked_fill(causal_mask == 0, float('-inf'))\n",
    "\n",
    "        att = nn.functional.softmax(att, dim=-1)\n",
    "        y = torch.matmul(att, v_projected)  # (B, n_head, T, head_size)\n",
    "\n",
    "        # reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.827839Z",
     "iopub.execute_input": "2025-02-22T15:11:49.828295Z",
     "iopub.status.idle": "2025-02-22T15:11:49.839855Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.828260Z",
     "shell.execute_reply": "2025-02-22T15:11:49.838611Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.310589Z",
     "start_time": "2025-02-22T16:49:57.305738Z"
    }
   },
   "outputs": [],
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "source": "## Performer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class CausalPerformerAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.n_features = config.attention_config.get('performer_features', 64) \\\n",
    "            if config.attention_config else 64\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=True)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=True)\n",
    "\n",
    "        # random projection for performer\n",
    "        proj = torch.randn(self.head_size, self.n_features) * 0.1\n",
    "        self.register_buffer(\"proj\", proj)\n",
    "\n",
    "        # causal mask for safety (though the prefix-sum approach is also doing it)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # random feature mapping\n",
    "        k_prime = self._prime(k)  # (B, n_head, T, n_features)\n",
    "        q_prime = self._prime(q)\n",
    "\n",
    "        # prefix sums along T\n",
    "        kprime_v = k_prime.unsqueeze(-1) * v.unsqueeze(-2)   # (B, n_head, T, n_features, head_size)\n",
    "        prefix_k = torch.cumsum(k_prime, dim=2)              # (B, n_head, T, n_features)\n",
    "        prefix_kv = torch.cumsum(kprime_v, dim=2)            # (B, n_head, T, n_features, head_size)\n",
    "\n",
    "        # numerator: q_prime[t, :] dot prefix_kv[t, :, :]\n",
    "        numerator = torch.einsum('b n t f, b n t f d -> b n t d', q_prime, prefix_kv)\n",
    "        # denominator\n",
    "        denominator = torch.einsum('b n t f, b n t f -> b n t', q_prime, prefix_k) + 1e-6\n",
    "\n",
    "        out = numerator / denominator.unsqueeze(-1)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(out)\n",
    "\n",
    "    def _prime(self, x):\n",
    "        # phi(x) = exp(xW - ||x||^2/2) / sqrt(n_features)\n",
    "        norm_sq = torch.sum(x**2, dim=-1, keepdim=True)\n",
    "        x_proj = torch.einsum('b n t d, d f -> b n t f', x, self.proj)\n",
    "        x_exp = torch.exp(x_proj - 0.5 * norm_sq)\n",
    "        return x_exp * (1.0 / math.sqrt(self.n_features))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.841193Z",
     "iopub.execute_input": "2025-02-22T15:11:49.841497Z",
     "iopub.status.idle": "2025-02-22T15:11:49.863809Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.841472Z",
     "shell.execute_reply": "2025-02-22T15:11:49.862678Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.327415Z",
     "start_time": "2025-02-22T16:49:57.322489Z"
    }
   },
   "outputs": [],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "source": "# Config",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class GPTConfig:\n",
    "    block_size: int = 4096\n",
    "    vocab_size: int = 65  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    attention_config = {'nystrom_landmarks': 64}\n",
    "    bias = 0.2"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:21:31.145958Z",
     "iopub.execute_input": "2025-02-22T15:21:31.146278Z",
     "iopub.status.idle": "2025-02-22T15:21:31.151653Z",
     "shell.execute_reply.started": "2025-02-22T15:21:31.146253Z",
     "shell.execute_reply": "2025-02-22T15:21:31.150132Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.343303Z",
     "start_time": "2025-02-22T16:49:57.341144Z"
    }
   },
   "outputs": [],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "source": "# Util",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def prepare_tokens(x, config, wte, wpe, ln):\n    # In: token embeddings of shape (1, t)\n    b, t = x.size()\n    pos = torch.arange(0, t, dtype=torch.long, device='cpu').unsqueeze(0)  # shape (1, t)\n    tok_emb = wte(x)  # token embeddings of shape (b, t, n_embd)\n    pos_emb = wpe(pos)  # position embeddings of shape (1, t, n_embd)\n    coded_x = tok_emb + pos_emb\n    norm_x = ln(coded_x)\n    return norm_x # token embeddings of shape (1, t, n_embd)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.894022Z",
     "iopub.execute_input": "2025-02-22T15:11:49.894412Z",
     "iopub.status.idle": "2025-02-22T15:11:49.914979Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.894362Z",
     "shell.execute_reply": "2025-02-22T15:11:49.913707Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.347523Z",
     "start_time": "2025-02-22T16:49:57.345699Z"
    }
   },
   "outputs": [],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "source": "def get_config(block_size):\n    config = GPTConfig()\n    config.block_size = block_size\n\n    wte=nn.Embedding(config.vocab_size, config.n_embd)\n    wpe=nn.Embedding(config.block_size, config.n_embd)\n    ln = nn.LayerNorm(config.n_embd, bias=False)\n\n    return config, wte, wpe, ln",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.916239Z",
     "iopub.execute_input": "2025-02-22T15:11:49.916785Z",
     "iopub.status.idle": "2025-02-22T15:11:49.937849Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.916745Z",
     "shell.execute_reply": "2025-02-22T15:11:49.936273Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.356377Z",
     "start_time": "2025-02-22T16:49:57.354439Z"
    }
   },
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "source": "def get_tokens(x_np_array, config, wte, wpe, ln):\n    print(f'Tokens shape (batch_size, context_window): {x_np_array.shape}')\n    x = torch.stack([torch.from_numpy(tokens) for tokens in x_np_array])\n    return prepare_tokens(x, config, wte, wpe, ln)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.939827Z",
     "iopub.execute_input": "2025-02-22T15:11:49.940182Z",
     "iopub.status.idle": "2025-02-22T15:11:49.962070Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.940146Z",
     "shell.execute_reply": "2025-02-22T15:11:49.960852Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.365451Z",
     "start_time": "2025-02-22T16:49:57.363382Z"
    }
   },
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "source": "with torch.no_grad():\n    def get_attention_matrix(att_type, block_size, x):\n        torch.manual_seed(42)\n        np.random.seed(42)\n        config, wte, wpe, ln = get_config(block_size)\n    \n        if att_type == 'linformer':\n            attention = LinformerAttention(config)\n        elif att_type == 'nystrom':\n            attention = NystromAttention(config)\n        elif att_type == 'vanilla':\n            attention = VanillaAttention(config)\n        elif att_type == 'performer':\n            attention = CausalPerformerAttention(config)\n    \n        tokens = get_tokens(x[:1, :block_size], config, wte, wpe, ln)\n        att_matrix = attention.forward(tokens)\n        print(f'obtained attention matrix of shape {att_matrix.shape}')\n        att_matrix = att_matrix\n            \n        return att_matrix",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:19:28.667390Z",
     "iopub.execute_input": "2025-02-22T15:19:28.667794Z",
     "iopub.status.idle": "2025-02-22T15:19:28.674094Z",
     "shell.execute_reply.started": "2025-02-22T15:19:28.667763Z",
     "shell.execute_reply": "2025-02-22T15:19:28.672956Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.374728Z",
     "start_time": "2025-02-22T16:49:57.372141Z"
    }
   },
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "source": "# Metrics",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.991965Z",
     "iopub.execute_input": "2025-02-22T15:11:49.992319Z",
     "iopub.status.idle": "2025-02-22T15:11:50.013101Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.992287Z",
     "shell.execute_reply": "2025-02-22T15:11:50.012101Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.383115Z",
     "start_time": "2025-02-22T16:49:57.381441Z"
    }
   },
   "outputs": [],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "source": [
    "x_array = np.loadtxt('./shakespeare_char/tokens10k.txt', delimiter=',')\n",
    "x_np = np.array([x_array]).astype(np.int64)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:50.014220Z",
     "iopub.execute_input": "2025-02-22T15:11:50.014483Z",
     "iopub.status.idle": "2025-02-22T15:11:50.046210Z",
     "shell.execute_reply.started": "2025-02-22T15:11:50.014460Z",
     "shell.execute_reply": "2025-02-22T15:11:50.044823Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.393215Z",
     "start_time": "2025-02-22T16:49:57.390189Z"
    }
   },
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "attention_type = 'vanilla'\n",
    "block_size = 4096\n",
    "attention_matrix = get_attention_matrix(attention_type, block_size, x_np)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:16:03.381384Z",
     "iopub.execute_input": "2025-02-22T15:16:03.381761Z",
     "iopub.status.idle": "2025-02-22T15:16:03.406380Z",
     "shell.execute_reply.started": "2025-02-22T15:16:03.381731Z",
     "shell.execute_reply": "2025-02-22T15:16:03.404942Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:57.465360Z",
     "start_time": "2025-02-22T16:49:57.399995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape (batch_size, context_window): (1, 4096)\n",
      "obtained attention matrix of shape torch.Size([1, 4096, 384])\n",
      "CPU times: user 304 ms, sys: 57.9 ms, total: 362 ms\n",
      "Wall time: 63.4 ms\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "source": [
    "vanilla_out = get_attention_matrix('vanilla', block_size, x_np).clone().detach()\n",
    "\n",
    "# compare with Nystrom\n",
    "nystrom_out = get_attention_matrix('nystrom', block_size, x_np).clone().detach()\n",
    "sim_nys = nn.functional.cosine_similarity(vanilla_out.view(-1), nystrom_out.view(-1), dim=0)\n",
    "print(f\"Vanilla vs Nystrom similarity: {sim_nys.item():.4f}\")\n",
    "\n",
    "# compare with Linformer\n",
    "linformer_out = get_attention_matrix('linformer', block_size, x_np).clone().detach()\n",
    "sim_lin = nn.functional.cosine_similarity(vanilla_out.view(-1), linformer_out.view(-1), dim=0)\n",
    "print(f\"Vanilla vs Linformer similarity: {sim_lin.item():.4f}\")\n",
    "\n",
    "# compare with Performer\n",
    "performer_out = get_attention_matrix('performer', block_size, x_np).clone().detach()\n",
    "sim_per = nn.functional.cosine_similarity(vanilla_out.view(-1), performer_out.view(-1), dim=0)\n",
    "print(f\"Vanilla vs Performer similarity: {sim_per.item():.4f}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:22:28.379149Z",
     "iopub.execute_input": "2025-02-22T15:22:28.379467Z",
     "iopub.status.idle": "2025-02-22T15:22:38.116604Z",
     "shell.execute_reply.started": "2025-02-22T15:22:28.379441Z",
     "shell.execute_reply": "2025-02-22T15:22:38.115499Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:58.140972Z",
     "start_time": "2025-02-22T16:49:57.473966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape (batch_size, context_window): (1, 4096)\n",
      "obtained attention matrix of shape torch.Size([1, 4096, 384])\n",
      "Nystrom with landmarks 64\n",
      "Tokens shape (batch_size, context_window): (1, 4096)\n",
      "obtained attention matrix of shape torch.Size([1, 4096, 384])\n",
      "Vanilla vs Nystrom similarity: 0.7488\n",
      "Tokens shape (batch_size, context_window): (1, 4096)\n",
      "obtained attention matrix of shape torch.Size([1, 4096, 384])\n",
      "Vanilla vs Linformer similarity: 0.9479\n",
      "Tokens shape (batch_size, context_window): (1, 4096)\n",
      "obtained attention matrix of shape torch.Size([1, 4096, 384])\n",
      "Vanilla vs Performer similarity: 0.7824\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "source": "config, wte, wpe, ln = get_config(256)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:50.132738Z",
     "iopub.execute_input": "2025-02-22T15:11:50.133137Z",
     "iopub.status.idle": "2025-02-22T15:11:50.140201Z",
     "shell.execute_reply.started": "2025-02-22T15:11:50.133086Z",
     "shell.execute_reply": "2025-02-22T15:11:50.138981Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:58.152275Z",
     "start_time": "2025-02-22T16:49:58.148559Z"
    }
   },
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "source": "x = torch.stack([torch.from_numpy(tokens) for tokens in x_np])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:50.141342Z",
     "iopub.execute_input": "2025-02-22T15:11:50.141717Z",
     "iopub.status.idle": "2025-02-22T15:11:50.160451Z",
     "shell.execute_reply.started": "2025-02-22T15:11:50.141687Z",
     "shell.execute_reply": "2025-02-22T15:11:50.159154Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:58.162449Z",
     "start_time": "2025-02-22T16:49:58.160148Z"
    }
   },
   "outputs": [],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "source": "wte(x)[0][:5][:5]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:50.161752Z",
     "iopub.execute_input": "2025-02-22T15:11:50.162131Z",
     "iopub.status.idle": "2025-02-22T15:11:50.195181Z",
     "shell.execute_reply.started": "2025-02-22T15:11:50.162090Z",
     "shell.execute_reply": "2025-02-22T15:11:50.194175Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-22T16:49:58.174280Z",
     "start_time": "2025-02-22T16:49:58.170474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5744, -0.3336,  1.2779,  ..., -0.4308,  0.8104,  2.1108],\n",
       "        [ 0.2076, -0.2405,  0.5607,  ...,  0.1064,  0.9199,  2.4935],\n",
       "        [-1.2587,  0.3004, -0.1708,  ...,  0.3788, -0.2477,  0.1624],\n",
       "        [ 0.8966, -0.5991, -0.1178,  ...,  1.4480, -0.7148,  0.1802],\n",
       "        [-0.0349,  1.2517, -0.3017,  ..., -1.5413, -0.3463,  0.0140]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  }
 ]
}
