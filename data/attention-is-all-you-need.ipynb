{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10817521,"sourceType":"datasetVersion","datasetId":6716316}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nimport math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional\nfrom torch.nn import functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.725968Z","iopub.execute_input":"2025-02-22T15:11:49.726331Z","iopub.status.idle":"2025-02-22T15:11:49.732004Z","shell.execute_reply.started":"2025-02-22T15:11:49.726302Z","shell.execute_reply":"2025-02-22T15:11:49.730391Z"}},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":"# Attentions","metadata":{}},{"cell_type":"markdown","source":"## Vanilla","metadata":{}},{"cell_type":"code","source":"class VanillaAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        \n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.block_size = config.block_size\n        \n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                 .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n                                                               is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = nn.functional.softmax(att, dim=-1)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n            \n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y)\n        return y ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.733557Z","iopub.execute_input":"2025-02-22T15:11:49.734058Z","iopub.status.idle":"2025-02-22T15:11:49.761872Z","shell.execute_reply.started":"2025-02-22T15:11:49.734017Z","shell.execute_reply":"2025-02-22T15:11:49.759956Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"## Nystrom","metadata":{}},{"cell_type":"code","source":"class NystromAttention(nn.Module):\n    \"\"\"\n    Linformer self-attention mechanism with linear complexity.\n    Projects keys and values to a lower dimensional space for efficiency.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        # Default Linformer config\n        self.n_landmarks = config.attention_config.get('nystrom_landmarks', 32) if config.attention_config else 32\n        print(f'Nystrom with landmarks {self.n_landmarks}')\n\n        # key, query, value projections for all heads\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # regularization\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        \n        # mask\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n        hs = C // self.n_head\n\n        # calculate query, key, values for all heads in batch\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n\n        k = k.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n        \n        params = {'B': B, 'nh': self.n_head, 'T': T, 'hs': hs}\n        \n        # Project keys and values to lower dimensional space\n        q_landmarks = self.__get_landmark_representation(q, self.n_landmarks, **params)\n        k_landmarks = self.__get_landmark_representation(k, self.n_landmarks, **params)\n        \n        # Compute the attention matrix\n        L = F.softmax(q @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1)\n        P = self.__iterative_inv(F.softmax(q_landmarks @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1))\n\n        N_prod = (q_landmarks @ k.transpose(-1, -2))\n        # print(N_prod.shape)\n        # print(self.bias.shape)\n        # print(q_landmarks.shape)\n        N_masked = N_prod.masked_fill(self.bias[:, :, :self.n_landmarks, :T] == 0, float('-inf'))\n        N = F.softmax(N_masked / math.sqrt(hs), dim=-1)\n        \n        # Compute attention scores\n        att = L @ P @ N\n\n        # Apply attention to values and reshape\n        y = att @ v  # (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        return self.c_proj(y)\n    \n    def __get_landmark_representation(self, tensor, num_landmarks, B, nh, T, hs):\n        tensor_reshaped = tensor.reshape(-1, nh, num_landmarks, T // num_landmarks, hs) # (B, nh, T, hs)\n        tensor_landmarks = tensor_reshaped.mean(dim=-2)\n        return tensor_landmarks\n\n    def __iterative_inv(self, mat, n_iter=6):\n        I = torch.eye(mat.size(-1), device=mat.device)\n        K = mat\n\n        # The entries of K are positive and ||K||_{\\infty} = 1 due to softmax\n        V = 1 / torch.max(torch.sum(K, dim=-2), dim = -1).values[:, :, None, None] * K.transpose(-1, -2)\n\n        for _ in range(n_iter):\n            KV = torch.matmul(K, V)\n            V = torch.matmul(0.25 * V, 13 * I - torch.matmul(KV, 15 * I - torch.matmul(KV, 7 * I - KV)))\n        return V","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:18:45.393591Z","iopub.execute_input":"2025-02-22T15:18:45.393946Z","iopub.status.idle":"2025-02-22T15:18:45.409608Z","shell.execute_reply.started":"2025-02-22T15:18:45.393917Z","shell.execute_reply":"2025-02-22T15:18:45.408276Z"}},"outputs":[],"execution_count":109},{"cell_type":"markdown","source":"## Linformer","metadata":{}},{"cell_type":"code","source":"class LinformerAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        self.n_head = config.n_head\n        self.head_size = config.n_embd // config.n_head\n        self.linformer_k = config.attention_config.get('linformer_k', config.block_size // 4) \\\n            if config.attention_config else config.block_size // 4\n\n        # Q, K, V projections\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # Linformer projection matrices E, F to go from T -> k\n        # shape (n_head, linformer_k, block_size)\n        self.E = nn.Parameter(torch.randn(config.n_head, self.linformer_k, config.block_size))\n        self.F = nn.Parameter(torch.randn(config.n_head, self.linformer_k, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        # (B, T, 3*n_embd) -> split -> each (B, T, n_embd)\n        q, k, v = self.c_attn(x).split(C, dim=2)\n\n        # shape them into heads\n        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, n_head, T, head_size)\n        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, n_head, T, head_size)\n        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, n_head, T, head_size)\n\n        # Project K and V from T->k\n        # E,F each is (n_head, linformer_k, block_size), slice to current T\n        E = self.E[:, :, :T]  # => (n_head, linformer_k, T)\n        F = self.F[:, :, :T]  # => (n_head, linformer_k, T)\n\n        # Now do the batch multiplication:\n        k_projected = torch.einsum('hkt, bhtd -> bhkd', E, k)  # (B, n_head, linformer_k, head_size)\n        v_projected = torch.einsum('hkt, bhtd -> bhkd', F, v)  # (B, n_head, linformer_k, head_size)\n\n        # Compute attention\n        # q is (B, n_head, T, head_size)\n        # k_projected is (B, n_head, linformer_k, head_size)\n        # so q @ k_projected^T => (B, n_head, T, linformer_k)\n        att = torch.matmul(\n            q, k_projected.transpose(-2, -1)\n        ) * (1.0 / math.sqrt(self.head_size))\n\n        att = nn.functional.softmax(att, -1)\n\n        # Then multiply by v_projected => (B, n_head, T, head_size)\n        y = att @ v_projected\n\n        # re-combine heads\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # final linear out\n        y = self.c_proj(y)\n        return y ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.827839Z","iopub.execute_input":"2025-02-22T15:11:49.828295Z","iopub.status.idle":"2025-02-22T15:11:49.839855Z","shell.execute_reply.started":"2025-02-22T15:11:49.828260Z","shell.execute_reply":"2025-02-22T15:11:49.838611Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"## Performer","metadata":{}},{"cell_type":"code","source":"class CausalPerformerAttention(nn.Module):\n    \"\"\"\n    Causal Performer attention (FAVOR+) for GPT-style models.\n    Uses random feature maps + prefix sums to enforce autoregressive masking.\n\n    NOTE:\n    - This is a *vectorized* implementation. For long T,\n      memory consumption can be high (we store prefix sums of shape ~ (B, n_head, T, ...)).\n    - If you need incremental generation, you would maintain prefix sums\n      in a stateful manner instead of computing them for the entire sequence at once.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_size = self.n_embd // self.n_head\n\n        # Number of random features for Performer\n        self.n_features = config.attention_config.get('performer_features', 64) \\\n            if config.attention_config else 64\n\n        # Q, K, V projections (linear)\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # Output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # Create a random projection matrix for the feature map\n        # Typically, it is not trained; we register it as a buffer\n        # shape = (head_size, n_features)\n        # scaled by e.g. 0.01 or 0.1 to keep exponent magnitudes stable\n        proj = torch.randn(self.head_size, self.n_features) * 0.1\n        self.register_buffer(\"proj\", proj)\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, T, C), where C = n_embd\n        returns: (B, T, C)\n        \"\"\"\n        B, T, C = x.size()\n\n        # 1) Compute Q, K, V in one fused linear op\n        qkv = self.c_attn(x)  # (B, T, 3*C)\n        q, k, v = qkv.split(C, dim=2)  # each is (B, T, C)\n\n        # 2) Reshape into heads: (B, n_head, T, head_size)\n        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n\n        # 3) Map K, Q via the Performer random feature map: phi(k), phi(q)\n        k_prime = self._prime(k)  # (B, n_head, T, n_features)\n        q_prime = self._prime(q)  # (B, n_head, T, n_features)\n\n        # 4) Compute prefix sums for enforcing causality.\n        #    We'll do cumulative sums along time dimension, so each position t\n        #    sees only sum_{<=t} instead of sum_{<=T}.\n\n        # 4a) Compute k_prime * v => shape (B, n_head, T, n_features, head_size)\n        #    We'll do an unsqueeze on the last dimension of k_prime or v\n        #    to match them up.\n        k_prime_expanded = k_prime.unsqueeze(-1)  # (B, n_head, T, n_features, 1)\n        v_expanded = v.unsqueeze(-2)  # (B, n_head, T, 1, head_size)\n        kprime_v = k_prime_expanded * v_expanded  # (B, n_head, T, n_features, head_size)\n\n        # 4b) prefix sums along T\n        prefix_k = torch.cumsum(k_prime, dim=2)  # (B, n_head, T, n_features)\n        prefix_kprime_v = torch.cumsum(kprime_v, dim=2)  # (B, n_head, T, n_features, head_size)\n\n        # 5) For each position t, the attention result is:\n        #    numerator[t]   = q_prime[t] dot prefix_kprime_v[t]\n        #    denominator[t] = q_prime[t] dot prefix_k[t]\n        #    out[t] = numerator[t] / denominator[t]\n\n        # We'll do that in a fully vectorized manner with einsum:\n        # numerator shape => (B, n_head, T, head_size)\n        numerator = torch.einsum(\n            'b n t f, b n t f d -> b n t d',  # q_prime[t,f] * prefix_kprime_v[t,f,d] -> out[t,d]\n            q_prime,\n            prefix_kprime_v\n        )\n        # denominator shape => (B, n_head, T)\n        denominator = torch.einsum(\n            'b n t f, b n t f -> b n t',  # q_prime[t,f] * prefix_k[t,f] -> scalar\n            q_prime,\n            prefix_k\n        ) + 1e-6  # avoid division by zero\n\n        out = numerator / denominator.unsqueeze(-1)  # broadcast over 'd'\n\n        # 7) Re-combine the heads: (B, T, C)\n        out = out.transpose(1, 2).contiguous().view(B, T, C)\n\n        # 8) Final linear projection\n        out = self.c_proj(out)\n        return out\n\n    def _prime(self, x):\n        \"\"\"\n        Performer random feature map:\n           phi(x) = exp(x * W - ||x||^2 / 2) / sqrt(n_features)\n        where W is self.proj (shape [head_size, n_features]).\n\n        x: shape (B, n_head, T, head_size)\n        returns: (B, n_head, T, n_features)\n        \"\"\"\n        # squared norm of x => (B, n_head, T, 1)\n        norm_sq = torch.sum(x ** 2, dim=-1, keepdim=True)  # ||x||^2\n\n        # x_proj => (B, n_head, T, n_features)\n        x_proj = torch.einsum('b n t d, d f -> b n t f', x, self.proj)\n\n        # exponent => exp(x_proj - norm_sq/2)\n        x_exp = torch.exp(x_proj - 0.5 * norm_sq)\n\n        # scale by 1 / sqrt(n_features)\n        x_exp = x_exp * (1.0 / math.sqrt(self.n_features))\n        return x_exp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.841193Z","iopub.execute_input":"2025-02-22T15:11:49.841497Z","iopub.status.idle":"2025-02-22T15:11:49.863809Z","shell.execute_reply.started":"2025-02-22T15:11:49.841472Z","shell.execute_reply":"2025-02-22T15:11:49.862678Z"}},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 65  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 6\n    n_head: int = 6\n    n_embd: int = 384\n    attention_config = {'nystrom_landmarks': 64}\n    bias = 0.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:21:31.145958Z","iopub.execute_input":"2025-02-22T15:21:31.146278Z","iopub.status.idle":"2025-02-22T15:21:31.151653Z","shell.execute_reply.started":"2025-02-22T15:21:31.146253Z","shell.execute_reply":"2025-02-22T15:21:31.150132Z"}},"outputs":[],"execution_count":122},{"cell_type":"markdown","source":"# Util","metadata":{}},{"cell_type":"code","source":"def prepare_tokens(x, config, wte, wpe, ln):\n    # In: token embeddings of shape (1, t)\n    b, t = x.size()\n    pos = torch.arange(0, t, dtype=torch.long, device='cpu').unsqueeze(0)  # shape (1, t)\n    tok_emb = wte(x)  # token embeddings of shape (b, t, n_embd)\n    pos_emb = wpe(pos)  # position embeddings of shape (1, t, n_embd)\n    coded_x = tok_emb + pos_emb\n    norm_x = ln(coded_x)\n    return norm_x # token embeddings of shape (1, t, n_embd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.894022Z","iopub.execute_input":"2025-02-22T15:11:49.894412Z","iopub.status.idle":"2025-02-22T15:11:49.914979Z","shell.execute_reply.started":"2025-02-22T15:11:49.894362Z","shell.execute_reply":"2025-02-22T15:11:49.913707Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"def get_config(block_size):\n    config = GPTConfig()\n    config.block_size = block_size\n\n    wte=nn.Embedding(config.vocab_size, config.n_embd)\n    wpe=nn.Embedding(config.block_size, config.n_embd)\n    ln = nn.LayerNorm(config.n_embd, bias=False)\n\n    return config, wte, wpe, ln","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.916239Z","iopub.execute_input":"2025-02-22T15:11:49.916785Z","iopub.status.idle":"2025-02-22T15:11:49.937849Z","shell.execute_reply.started":"2025-02-22T15:11:49.916745Z","shell.execute_reply":"2025-02-22T15:11:49.936273Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"def get_tokens(x_np_array, config, wte, wpe, ln):\n    print(f'Tokens shape (batch_size, context_window): {x_np_array.shape}')\n    x = torch.stack([torch.from_numpy(tokens) for tokens in x_np_array])\n    return prepare_tokens(x, config, wte, wpe, ln)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.939827Z","iopub.execute_input":"2025-02-22T15:11:49.940182Z","iopub.status.idle":"2025-02-22T15:11:49.962070Z","shell.execute_reply.started":"2025-02-22T15:11:49.940146Z","shell.execute_reply":"2025-02-22T15:11:49.960852Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"with torch.no_grad():\n    def get_attention_matrix(att_type, block_size, x):\n        torch.manual_seed(42)\n        np.random.seed(42)\n        config, wte, wpe, ln = get_config(block_size)\n    \n        if att_type == 'linformer':\n            attention = LinformerAttention(config)\n        elif att_type == 'nystrom':\n            attention = NystromAttention(config)\n        elif att_type == 'vanilla':\n            attention = VanillaAttention(config)\n        elif att_type == 'performer':\n            attention = CausalPerformerAttention(config)\n    \n        tokens = get_tokens(x[:1, :block_size], config, wte, wpe, ln)\n        att_matrix = attention.forward(tokens)\n        print(f'obtained attention matrix of shape {att_matrix.shape}')\n        att_matrix = att_matrix\n            \n        return att_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:19:28.667390Z","iopub.execute_input":"2025-02-22T15:19:28.667794Z","iopub.status.idle":"2025-02-22T15:19:28.674094Z","shell.execute_reply.started":"2025-02-22T15:19:28.667763Z","shell.execute_reply":"2025-02-22T15:19:28.672956Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"# Metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:49.991965Z","iopub.execute_input":"2025-02-22T15:11:49.992319Z","iopub.status.idle":"2025-02-22T15:11:50.013101Z","shell.execute_reply.started":"2025-02-22T15:11:49.992287Z","shell.execute_reply":"2025-02-22T15:11:50.012101Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"x_array = np.loadtxt('/kaggle/input/attention-tokens/tokens8k.txt', delimiter=',')\nx_np = np.array([x_array]).astype(np.int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:50.014220Z","iopub.execute_input":"2025-02-22T15:11:50.014483Z","iopub.status.idle":"2025-02-22T15:11:50.046210Z","shell.execute_reply.started":"2025-02-22T15:11:50.014460Z","shell.execute_reply":"2025-02-22T15:11:50.044823Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"%%time\nattention_type = 'vanilla'\nblock_size = 256\nattention_matrix = get_attention_matrix(attention_type, block_size, x_np)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:16:03.381384Z","iopub.execute_input":"2025-02-22T15:16:03.381761Z","iopub.status.idle":"2025-02-22T15:16:03.406380Z","shell.execute_reply.started":"2025-02-22T15:16:03.381731Z","shell.execute_reply":"2025-02-22T15:16:03.404942Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Tokens shape (batch_size, context_window): (1, 256)\nobtained attention matrix of shape torch.Size([1, 256, 384])\nCPU times: user 23.7 ms, sys: 1.12 ms, total: 24.8 ms\nWall time: 19.4 ms\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"vanilla_matrix1 = get_attention_matrix('vanilla', 8192, x_np).clone().detach()\nnystrom_matrix = get_attention_matrix('linformer', 8192, x_np).clone().detach()\nsim = F.cosine_similarity(vanilla_matrix1.view(-1), nystrom_matrix.view(-1), dim=0)\nprint(sim.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:22:28.379149Z","iopub.execute_input":"2025-02-22T15:22:28.379467Z","iopub.status.idle":"2025-02-22T15:22:38.116604Z","shell.execute_reply.started":"2025-02-22T15:22:28.379441Z","shell.execute_reply":"2025-02-22T15:22:38.115499Z"}},"outputs":[{"name":"stdout","text":"Tokens shape (batch_size, context_window): (1, 8192)\nobtained attention matrix of shape torch.Size([1, 8192, 384])\nTokens shape (batch_size, context_window): (1, 8192)\nobtained attention matrix of shape torch.Size([1, 8192, 384])\n-0.002036292804405093\n","output_type":"stream"}],"execution_count":126},{"cell_type":"code","source":"config, wte, wpe, ln = get_config(256)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:50.132738Z","iopub.execute_input":"2025-02-22T15:11:50.133137Z","iopub.status.idle":"2025-02-22T15:11:50.140201Z","shell.execute_reply.started":"2025-02-22T15:11:50.133086Z","shell.execute_reply":"2025-02-22T15:11:50.138981Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"x = torch.stack([torch.from_numpy(tokens) for tokens in x_np])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:50.141342Z","iopub.execute_input":"2025-02-22T15:11:50.141717Z","iopub.status.idle":"2025-02-22T15:11:50.160451Z","shell.execute_reply.started":"2025-02-22T15:11:50.141687Z","shell.execute_reply":"2025-02-22T15:11:50.159154Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"wte(x)[0][:5][:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:50.161752Z","iopub.execute_input":"2025-02-22T15:11:50.162131Z","iopub.status.idle":"2025-02-22T15:11:50.195181Z","shell.execute_reply.started":"2025-02-22T15:11:50.162090Z","shell.execute_reply":"2025-02-22T15:11:50.194175Z"}},"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.7175, -1.4476, -0.7953,  ..., -0.2047,  0.1062,  1.1078],\n        [-1.0297, -0.1968,  0.7316,  ..., -1.1973, -0.3622, -0.2030],\n        [-0.8943,  0.8867,  0.0396,  ...,  1.0203, -0.8696, -0.7008],\n        [ 0.3200, -0.2651, -0.0264,  ..., -2.3089,  0.4150,  0.3817],\n        [ 1.0118, -0.4148,  0.8462,  ..., -1.7770,  0.3753,  0.0305]],\n       grad_fn=<SliceBackward0>)"},"metadata":{}}],"execution_count":90},{"cell_type":"markdown","source":"","metadata":{}}]}