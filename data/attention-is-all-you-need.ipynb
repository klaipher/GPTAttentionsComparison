{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10817521,"sourceType":"datasetVersion","datasetId":6716316}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport time\nimport tracemalloc\n\nimport math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional\nfrom torch.nn import functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:18.476937Z","iopub.execute_input":"2025-02-24T12:36:18.477146Z","iopub.status.idle":"2025-02-24T12:36:21.710843Z","shell.execute_reply.started":"2025-02-24T12:36:18.477125Z","shell.execute_reply":"2025-02-24T12:36:21.709909Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Attentions","metadata":{}},{"cell_type":"markdown","source":"## Vanilla","metadata":{}},{"cell_type":"code","source":"class VanillaAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        \n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.block_size = config.block_size\n        \n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # manual implementation of attention\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = nn.functional.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n            \n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y)\n        return y ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:53.685127Z","iopub.execute_input":"2025-02-24T12:36:53.685468Z","iopub.status.idle":"2025-02-24T12:36:53.693711Z","shell.execute_reply.started":"2025-02-24T12:36:53.685440Z","shell.execute_reply":"2025-02-24T12:36:53.692887Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Nystrom","metadata":{}},{"cell_type":"code","source":"class NystromAttention(nn.Module):\n    \"\"\"\n    Linformer self-attention mechanism with linear complexity.\n    Projects keys and values to a lower dimensional space for efficiency.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        # Default Linformer config\n        self.n_landmarks = config.attention_config.get('nystrom_landmarks', 32) if config.attention_config else 32\n\n        # key, query, value projections for all heads\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # regularization\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        \n        # mask\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n        hs = C // self.n_head\n\n        # calculate query, key, values for all heads in batch\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n\n        k = k.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n        \n        params = {'B': B, 'nh': self.n_head, 'T': T, 'hs': hs}\n        \n        # Project keys and values to lower dimensional space\n        q_landmarks = self.__get_landmark_representation(q, self.n_landmarks, **params)\n        k_landmarks = self.__get_landmark_representation(k, self.n_landmarks, **params)\n        \n        # Compute the attention matrix\n        L = F.softmax(q @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1)\n        P = self.__iterative_inv(F.softmax(q_landmarks @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1))\n\n        N_prod = (q_landmarks @ k.transpose(-1, -2))\n        # print(N_prod.shape)\n        # print(self.bias.shape)\n        # print(q_landmarks.shape)\n        N_masked = N_prod.masked_fill(self.bias[:, :, :self.n_landmarks, :T] == 0, float('-inf'))\n        N = F.softmax(N_masked / math.sqrt(hs), dim=-1)\n        \n        # Compute attention scores\n        m1 = (L @ P)\n        m2 = (N @ v)\n        att = m1 @ m2 # (B, nh, T, hs)\n\n        # Apply attention to values and reshape\n        y = att\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        return self.c_proj(y)\n    \n    def __get_landmark_representation(self, tensor, num_landmarks, B, nh, T, hs):\n        tensor_reshaped = tensor.reshape(-1, nh, num_landmarks, T // num_landmarks, hs) # (B, nh, T, hs)\n        tensor_landmarks = tensor_reshaped.mean(dim=-2)\n        return tensor_landmarks\n\n    def __iterative_inv(self, mat, n_iter=6):\n        I = torch.eye(mat.size(-1), device=mat.device)\n        K = mat\n\n        # The entries of K are positive and ||K||_{\\infty} = 1 due to softmax\n        V = 1 / torch.max(torch.sum(K, dim=-2), dim = -1).values[:, :, None, None] * K.transpose(-1, -2)\n\n        for _ in range(n_iter):\n            KV = torch.matmul(K, V)\n            V = torch.matmul(0.25 * V, 13 * I - torch.matmul(KV, 15 * I - torch.matmul(KV, 7 * I - KV)))\n        return V","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:42:33.168096Z","iopub.execute_input":"2025-02-24T12:42:33.168403Z","iopub.status.idle":"2025-02-24T12:42:33.179899Z","shell.execute_reply.started":"2025-02-24T12:42:33.168377Z","shell.execute_reply":"2025-02-24T12:42:33.178897Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## Linformer","metadata":{}},{"cell_type":"code","source":"class LinformerAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        self.n_head = config.n_head\n        self.head_size = config.n_embd // config.n_head\n        self.linformer_k = config.attention_config.get('linformer_k', config.block_size // 4) \\\n            if config.attention_config else config.block_size // 4\n\n        # Q, K, V projections\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # Linformer projection matrices E, F to go from T -> k\n        # shape (n_head, linformer_k, block_size)\n        self.E = nn.Parameter(torch.randn(config.n_head, self.linformer_k, config.block_size))\n        self.F = nn.Parameter(torch.randn(config.n_head, self.linformer_k, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        # (B, T, 3*n_embd) -> split -> each (B, T, n_embd)\n        q, k, v = self.c_attn(x).split(C, dim=2)\n\n        # shape them into heads\n        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, n_head, T, head_size)\n        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, n_head, T, head_size)\n        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, n_head, T, head_size)\n\n        # Project K and V from T->k\n        # E,F each is (n_head, linformer_k, block_size), slice to current T\n        E = self.E[:, :, :T]  # => (n_head, linformer_k, T)\n        F = self.F[:, :, :T]  # => (n_head, linformer_k, T)\n\n        # Now do the batch multiplication:\n        k_projected = torch.einsum('hkt, bhtd -> bhkd', E, k)  # (B, n_head, linformer_k, head_size)\n        v_projected = torch.einsum('hkt, bhtd -> bhkd', F, v)  # (B, n_head, linformer_k, head_size)\n\n        # Compute attention\n        # q is (B, n_head, T, head_size)\n        # k_projected is (B, n_head, linformer_k, head_size)\n        # so q @ k_projected^T => (B, n_head, T, linformer_k)\n        att = torch.matmul(\n            q, k_projected.transpose(-2, -1)\n        ) * (1.0 / math.sqrt(self.head_size))\n\n        att = nn.functional.softmax(att, -1)\n\n        # Then multiply by v_projected => (B, n_head, T, head_size)\n        y = att @ v_projected\n\n        # re-combine heads\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # final linear out\n        y = self.c_proj(y)\n        return y ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:21.746944Z","iopub.execute_input":"2025-02-24T12:36:21.747136Z","iopub.status.idle":"2025-02-24T12:36:21.769668Z","shell.execute_reply.started":"2025-02-24T12:36:21.747119Z","shell.execute_reply":"2025-02-24T12:36:21.768735Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Performer","metadata":{}},{"cell_type":"code","source":"class CausalPerformerAttention(nn.Module):\n    \"\"\"\n    Causal Performer attention (FAVOR+) for GPT-style models.\n    Uses random feature maps + prefix sums to enforce autoregressive masking.\n\n    NOTE:\n    - This is a *vectorized* implementation. For long T,\n      memory consumption can be high (we store prefix sums of shape ~ (B, n_head, T, ...)).\n    - If you need incremental generation, you would maintain prefix sums\n      in a stateful manner instead of computing them for the entire sequence at once.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_size = self.n_embd // self.n_head\n\n        # Number of random features for Performer\n        self.n_features = config.attention_config.get('performer_features', 64) \\\n            if config.attention_config else 64\n\n        # Q, K, V projections (linear)\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # Output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # Create a random projection matrix for the feature map\n        # Typically, it is not trained; we register it as a buffer\n        # shape = (head_size, n_features)\n        # scaled by e.g. 0.01 or 0.1 to keep exponent magnitudes stable\n        proj = torch.randn(self.head_size, self.n_features) * 0.1\n        self.register_buffer(\"proj\", proj)\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, T, C), where C = n_embd\n        returns: (B, T, C)\n        \"\"\"\n        B, T, C = x.size()\n\n        # 1) Compute Q, K, V in one fused linear op\n        qkv = self.c_attn(x)  # (B, T, 3*C)\n        q, k, v = qkv.split(C, dim=2)  # each is (B, T, C)\n\n        # 2) Reshape into heads: (B, n_head, T, head_size)\n        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n\n        # 3) Map K, Q via the Performer random feature map: phi(k), phi(q)\n        k_prime = self._prime(k)  # (B, n_head, T, n_features)\n        q_prime = self._prime(q)  # (B, n_head, T, n_features)\n\n        # 4) Compute prefix sums for enforcing causality.\n        #    We'll do cumulative sums along time dimension, so each position t\n        #    sees only sum_{<=t} instead of sum_{<=T}.\n\n        # 4a) Compute k_prime * v => shape (B, n_head, T, n_features, head_size)\n        #    We'll do an unsqueeze on the last dimension of k_prime or v\n        #    to match them up.\n        k_prime_expanded = k_prime.unsqueeze(-1)  # (B, n_head, T, n_features, 1)\n        v_expanded = v.unsqueeze(-2)  # (B, n_head, T, 1, head_size)\n        kprime_v = k_prime_expanded * v_expanded  # (B, n_head, T, n_features, head_size)\n\n        # 4b) prefix sums along T\n        prefix_k = torch.cumsum(k_prime, dim=2)  # (B, n_head, T, n_features)\n        prefix_kprime_v = torch.cumsum(kprime_v, dim=2)  # (B, n_head, T, n_features, head_size)\n\n        # 5) For each position t, the attention result is:\n        #    numerator[t]   = q_prime[t] dot prefix_kprime_v[t]\n        #    denominator[t] = q_prime[t] dot prefix_k[t]\n        #    out[t] = numerator[t] / denominator[t]\n\n        # We'll do that in a fully vectorized manner with einsum:\n        # numerator shape => (B, n_head, T, head_size)\n        numerator = torch.einsum(\n            'b n t f, b n t f d -> b n t d',  # q_prime[t,f] * prefix_kprime_v[t,f,d] -> out[t,d]\n            q_prime,\n            prefix_kprime_v\n        )\n        # denominator shape => (B, n_head, T)\n        denominator = torch.einsum(\n            'b n t f, b n t f -> b n t',  # q_prime[t,f] * prefix_k[t,f] -> scalar\n            q_prime,\n            prefix_k\n        ) + 1e-6  # avoid division by zero\n\n        out = numerator / denominator.unsqueeze(-1)  # broadcast over 'd'\n\n        # 7) Re-combine the heads: (B, T, C)\n        out = out.transpose(1, 2).contiguous().view(B, T, C)\n\n        # 8) Final linear projection\n        out = self.c_proj(out)\n        return out\n\n    def _prime(self, x):\n        \"\"\"\n        Performer random feature map:\n           phi(x) = exp(x * W - ||x||^2 / 2) / sqrt(n_features)\n        where W is self.proj (shape [head_size, n_features]).\n\n        x: shape (B, n_head, T, head_size)\n        returns: (B, n_head, T, n_features)\n        \"\"\"\n        # squared norm of x => (B, n_head, T, 1)\n        norm_sq = torch.sum(x ** 2, dim=-1, keepdim=True)  # ||x||^2\n\n        # x_proj => (B, n_head, T, n_features)\n        x_proj = torch.einsum('b n t d, d f -> b n t f', x, self.proj)\n\n        # exponent => exp(x_proj - norm_sq/2)\n        x_exp = torch.exp(x_proj - 0.5 * norm_sq)\n\n        # scale by 1 / sqrt(n_features)\n        x_exp = x_exp * (1.0 / math.sqrt(self.n_features))\n        return x_exp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:21.771079Z","iopub.execute_input":"2025-02-24T12:36:21.771328Z","iopub.status.idle":"2025-02-24T12:36:21.792827Z","shell.execute_reply.started":"2025-02-24T12:36:21.771307Z","shell.execute_reply":"2025-02-24T12:36:21.791957Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 65  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 6\n    n_head: int = 6\n    n_embd: int = 384\n    attention_config = {'nystrom_landmarks': 64}\n    bias = 0.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:21.793817Z","iopub.execute_input":"2025-02-24T12:36:21.794155Z","iopub.status.idle":"2025-02-24T12:36:21.811590Z","shell.execute_reply.started":"2025-02-24T12:36:21.794125Z","shell.execute_reply":"2025-02-24T12:36:21.810835Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Util","metadata":{}},{"cell_type":"code","source":"def prepare_tokens(x, config, wte, wpe, ln):\n    # In: token embeddings of shape (1, t)\n    b, t = x.size()\n    pos = torch.arange(0, t, dtype=torch.long, device='cpu').unsqueeze(0)  # shape (1, t)\n    tok_emb = wte(x)  # token embeddings of shape (b, t, n_embd)\n    pos_emb = wpe(pos)  # position embeddings of shape (1, t, n_embd)\n    coded_x = tok_emb + pos_emb\n    norm_x = ln(coded_x)\n    return norm_x # token embeddings of shape (1, t, n_embd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:21.812515Z","iopub.execute_input":"2025-02-24T12:36:21.812842Z","iopub.status.idle":"2025-02-24T12:36:21.830527Z","shell.execute_reply.started":"2025-02-24T12:36:21.812812Z","shell.execute_reply":"2025-02-24T12:36:21.829540Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_config(block_size, attention_config=None):\n    config = GPTConfig()\n    config.block_size = block_size\n    if attention_config:\n        config.attention_config = attention_config\n\n    wte=nn.Embedding(config.vocab_size, config.n_embd)\n    wpe=nn.Embedding(config.block_size, config.n_embd)\n    ln = nn.LayerNorm(config.n_embd, bias=False)\n\n    return config, wte, wpe, ln","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:21.831431Z","iopub.execute_input":"2025-02-24T12:36:21.831722Z","iopub.status.idle":"2025-02-24T12:36:21.845690Z","shell.execute_reply.started":"2025-02-24T12:36:21.831687Z","shell.execute_reply":"2025-02-24T12:36:21.844839Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def get_tokens(x_np_array, config, wte, wpe, ln):\n    # print(f'Tokens shape (batch_size, context_window): {x_np_array.shape}')\n    x = torch.stack([torch.from_numpy(tokens) for tokens in x_np_array])\n    return prepare_tokens(x, config, wte, wpe, ln)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:21.846473Z","iopub.execute_input":"2025-02-24T12:36:21.846805Z","iopub.status.idle":"2025-02-24T12:36:21.862536Z","shell.execute_reply.started":"2025-02-24T12:36:21.846777Z","shell.execute_reply":"2025-02-24T12:36:21.861622Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!pip install memory_profiler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:21.863368Z","iopub.execute_input":"2025-02-24T12:36:21.863685Z","iopub.status.idle":"2025-02-24T12:36:26.378422Z","shell.execute_reply.started":"2025-02-24T12:36:21.863662Z","shell.execute_reply":"2025-02-24T12:36:26.377571Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting memory_profiler\n  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\nDownloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\nInstalling collected packages: memory_profiler\nSuccessfully installed memory_profiler-0.61.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import time\nimport gc\nimport torch\n\nwith torch.no_grad():\n    def get_attention_matrix(att_type, block_size, x, attention_config=None):\n        torch.manual_seed(42)\n        np.random.seed(42)\n        config, wte, wpe, ln = get_config(block_size, attention_config)\n\n        if att_type == 'linformer':\n            attention = LinformerAttention(config)\n        elif att_type == 'nystrom':\n            attention = NystromAttention(config)\n        elif att_type == 'vanilla':\n            attention = VanillaAttention(config)\n        elif att_type == 'performer':\n            attention = CausalPerformerAttention(config)\n    \n        tokens = get_tokens(x[:1, :block_size], config, wte, wpe, ln)\n    \n        # Move models and tensors to CUDA\n        attention = attention.to('cuda')\n        tokens = tokens.to('cuda')\n        wte = wte.to('cuda')\n        wpe = wpe.to('cuda')\n        ln = ln.to('cuda')\n    \n        # Clear CUDA caches and run garbage collection to minimize interference\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n        # Reset CUDA memory stats and record the baseline allocated memory (in bytes)\n        torch.cuda.reset_peak_memory_stats()\n        baseline_cuda_mem = torch.cuda.memory_allocated()\n    \n        start_time = time.time()\n    \n        # Define and run the forward pass while optionally disabling Python GC\n        def forward_call():\n            gc.disable()  # Pause Python garbage collection\n            try:\n                return attention.forward(tokens)\n            finally:\n                gc.enable()  # Re-enable GC afterwards\n    \n        att_matrix = forward_call()\n    \n        # Synchronize to ensure all CUDA operations are finished\n        torch.cuda.synchronize()\n        elapsed_time = time.time() - start_time\n    \n        # Compute the peak CUDA memory allocated during forward (in bytes)\n        peak_cuda_mem = torch.cuda.max_memory_allocated() - baseline_cuda_mem\n    \n        # print(f'obtained attention matrix of shape {att_matrix.shape}')\n        # print(f'Time taken for attention.forward: {elapsed_time*100:.2f} ms')\n        # print(f'Peak CUDA memory usage during attention.forward: {peak_cuda_mem / (1024*1024):.2f} MB')\n    \n        # Optionally, if you wish to return the result while cleaning up CUDA memory,\n        # move the result back to CPU before deleting GPU references.\n        result = att_matrix.cpu()\n    \n        # Clean up GPU memory by deleting variables and clearing the CUDA cache.\n        del att_matrix, tokens, attention, wte, wpe, ln\n        torch.cuda.empty_cache()\n    \n        return result, elapsed_time, peak_cuda_mem","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:40:30.426383Z","iopub.execute_input":"2025-02-24T12:40:30.426772Z","iopub.status.idle":"2025-02-24T12:40:30.434371Z","shell.execute_reply.started":"2025-02-24T12:40:30.426725Z","shell.execute_reply":"2025-02-24T12:40:30.433295Z"}},"outputs":[],"execution_count":27},{"cell_type":"raw","source":"\n","metadata":{"execution":{"iopub.status.busy":"2025-02-24T12:36:26.379435Z","iopub.execute_input":"2025-02-24T12:36:26.379666Z","iopub.status.idle":"2025-02-24T12:36:26.388199Z","shell.execute_reply.started":"2025-02-24T12:36:26.379646Z","shell.execute_reply":"2025-02-24T12:36:26.387323Z"}}},{"cell_type":"code","source":"# Metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:26.389096Z","iopub.execute_input":"2025-02-24T12:36:26.389378Z","iopub.status.idle":"2025-02-24T12:36:26.406093Z","shell.execute_reply.started":"2025-02-24T12:36:26.389355Z","shell.execute_reply":"2025-02-24T12:36:26.405314Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"x_array = np.loadtxt('/kaggle/input/attention-tokens/tokens8k.txt', delimiter=',')\nx_np = np.array([x_array]).astype(np.int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:36:26.408691Z","iopub.execute_input":"2025-02-24T12:36:26.408945Z","iopub.status.idle":"2025-02-24T12:36:26.435021Z","shell.execute_reply.started":"2025-02-24T12:36:26.408925Z","shell.execute_reply":"2025-02-24T12:36:26.434319Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"vanilla_matrix1, elapsed_time_vanilla, peak_memory_vanilla = get_attention_matrix('vanilla', 8192, x_np)\nnystrom_matrix, elapsed_time_nystrom, peak_memory_nystrom = get_attention_matrix('nystrom', 8192, x_np, attention_config={'nystrom_landmarks': 64})\n# sim = F.cosine_similarity(vanilla_matrix1.view(-1), nystrom_matrix.view(-1), dim=0)\n# print(sim.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:39:14.668608Z","iopub.execute_input":"2025-02-24T12:39:14.668985Z","iopub.status.idle":"2025-02-24T12:39:15.762206Z","shell.execute_reply.started":"2025-02-24T12:39:14.668957Z","shell.execute_reply":"2025-02-24T12:39:15.761242Z"}},"outputs":[{"name":"stdout","text":"vanilla\nnystrom\nNystrom with landmarks 64\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport time\nimport gc\n\ndef collect_attention_metrics(x, num_runs=10):\n    \"\"\"\n    Collects time, CUDA memory, and cosine similarity metrics for different attention methods.\n    \n    For each block size in [256, 512, 1024, 2048, 4096, 8192] and each method \n    ('vanilla' maps to 'classic', plus 'nystrom', 'linformer', 'performer'), the function:\n      1. Computes a baseline vanilla attention matrix for that block size.\n      2. Runs 10 forward passes of the given method.\n      3. Computes the cosine similarity between the computed attention matrix and the baseline vanilla matrix.\n      4. Records the elapsed time and peak CUDA memory usage.\n    \n    Assumes get_attention_matrix(att_type, block_size, x, attention_config) is defined and runs on CUDA.\n    \n    Returns:\n        A dictionary structured as:\n        {\n            \"classic\": {\"time\": [..], \"memory\": [..], \"similarity\": [..]},\n            \"nystrom\": {\"time\": [..], \"memory\": [..], \"similarity\": [..]},\n            \"linformer\": {\"time\": [..], \"memory\": [..], \"similarity\": [..]},\n            \"performer\": {\"time\": [..], \"memory\": [..], \"similarity\": [..]}\n        }\n    \"\"\"\n    # Mapping: 'vanilla' attention corresponds to the \"classic\" baseline.\n    methods = {\n        \"vanilla\": \"vanilla\",\n        \"nystrom32\": \"nystrom\",\n        \"nystrom64\": \"nystrom\",\n        \"nystrom128\": \"nystrom\",\n        \"linformer256\": \"linformer\",\n        \"linformer512\": \"linformer\",\n        \"performer\": \"performer\"\n    }\n\n    configs = {\n        \"vanilla\": None,\n        \"nystrom32\": {'nystrom_landmarks': 32},\n        \"nystrom64\": {'nystrom_landmarks': 64},\n        \"nystrom128\": {'nystrom_landmarks': 128},\n        \"linformer256\": {'linformer_k': 256},\n        \"linformer512\": {'linformer_k': 512},\n        \"performer\": None\n    }\n    # Define the block sizes to test.\n    block_sizes = [256, 512, 1024, 2048, 4096, 8192]\n    \n    # Initialize results dictionary.\n    results = {}\n    \n    for block_size in block_sizes:\n        print(f\"Block size: {block_size}\")\n        results[block_size] = { key: {\"time\": [], \"memory\": [], \"similarity\": []} \n                                for key, _ in methods.items() }\n        # Compute baseline vanilla attention matrix once for this block size.\n        vanilla_matrix, _, _ = get_attention_matrix(\"vanilla\", block_size, x)\n        # Move baseline to CPU and flatten for similarity computation.\n        vanilla_flat = vanilla_matrix.cpu().flatten()\n        # Clean up GPU memory.\n        del vanilla_matrix\n        torch.cuda.empty_cache()\n        \n        # Loop over each attention method.\n        for att_type, result_key in methods.items():\n            print(f\"  Method: {att_type}\")\n            for run in range(num_runs):\n                # Run the forward pass and capture time and CUDA memory metrics.\n                att_matrix, run_time, run_memory = get_attention_matrix(result_key, block_size, x, configs[att_type])\n                # Move computed matrix to CPU and flatten.\n                att_flat = att_matrix.flatten()\n                # Compute cosine similarity between this result and the vanilla baseline.\n                cosine_sim = F.cosine_similarity(att_flat, vanilla_flat, dim=0).item()\n                \n                # Record the metrics.\n                results[block_size][att_type][\"time\"].append(run_time)\n                results[block_size][att_type][\"memory\"].append(run_memory)\n                results[block_size][att_type][\"similarity\"].append(cosine_sim)\n                \n                # Clean up the attention matrix from GPU memory.\n                del att_matrix, att_flat\n                torch.cuda.empty_cache()\n    \n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:46:53.251634Z","iopub.execute_input":"2025-02-24T12:46:53.252001Z","iopub.status.idle":"2025-02-24T12:46:53.260648Z","shell.execute_reply.started":"2025-02-24T12:46:53.251974Z","shell.execute_reply":"2025-02-24T12:46:53.259512Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"result = collect_attention_metrics(x_np)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:46:58.138566Z","iopub.execute_input":"2025-02-24T12:46:58.138923Z","iopub.status.idle":"2025-02-24T12:48:10.479292Z","shell.execute_reply.started":"2025-02-24T12:46:58.138895Z","shell.execute_reply":"2025-02-24T12:48:10.478599Z"}},"outputs":[{"name":"stdout","text":"Block size: 256\n  Method: vanilla\n  Method: nystrom32\n  Method: nystrom64\n  Method: nystrom128\n  Method: linformer256\n  Method: linformer512\n  Method: performer\nBlock size: 512\n  Method: vanilla\n  Method: nystrom32\n  Method: nystrom64\n  Method: nystrom128\n  Method: linformer256\n  Method: linformer512\n  Method: performer\nBlock size: 1024\n  Method: vanilla\n  Method: nystrom32\n  Method: nystrom64\n  Method: nystrom128\n  Method: linformer256\n  Method: linformer512\n  Method: performer\nBlock size: 2048\n  Method: vanilla\n  Method: nystrom32\n  Method: nystrom64\n  Method: nystrom128\n  Method: linformer256\n  Method: linformer512\n  Method: performer\nBlock size: 4096\n  Method: vanilla\n  Method: nystrom32\n  Method: nystrom64\n  Method: nystrom128\n  Method: linformer256\n  Method: linformer512\n  Method: performer\nBlock size: 8192\n  Method: vanilla\n  Method: nystrom32\n  Method: nystrom64\n  Method: nystrom128\n  Method: linformer256\n  Method: linformer512\n  Method: performer\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"result[8192]['vanilla']['memory']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:50:01.089201Z","iopub.execute_input":"2025-02-24T12:50:01.089493Z","iopub.status.idle":"2025-02-24T12:50:01.095281Z","shell.execute_reply.started":"2025-02-24T12:50:01.089472Z","shell.execute_reply":"2025-02-24T12:50:01.094283Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"[3326083072,\n 3326083072,\n 3326083072,\n 3326083072,\n 3326083072,\n 3326083072,\n 3326083072,\n 3326083072,\n 3326083072,\n 3326083072]"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"result[8192]['nystrom64']['time']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:50:42.452995Z","iopub.execute_input":"2025-02-24T12:50:42.453277Z","iopub.status.idle":"2025-02-24T12:50:42.458809Z","shell.execute_reply.started":"2025-02-24T12:50:42.453257Z","shell.execute_reply":"2025-02-24T12:50:42.457850Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"[0.006267547607421875,\n 0.006776332855224609,\n 0.005112886428833008,\n 0.0051805973052978516,\n 0.005142688751220703,\n 0.005255460739135742,\n 0.0052449703216552734,\n 0.005128383636474609,\n 0.005702018737792969,\n 0.005146980285644531]"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"(3326083072-144016896) /(1024*1024)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:50:34.478807Z","iopub.execute_input":"2025-02-24T12:50:34.479119Z","iopub.status.idle":"2025-02-24T12:50:34.484345Z","shell.execute_reply.started":"2025-02-24T12:50:34.479099Z","shell.execute_reply":"2025-02-24T12:50:34.483485Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"3034.65478515625"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"","metadata":{}}]}