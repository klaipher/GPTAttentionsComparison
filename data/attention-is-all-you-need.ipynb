{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 10817521,
     "sourceType": "datasetVersion",
     "datasetId": 6716316
    }
   ],
   "dockerImageVersionId": 30919,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.725968Z",
     "iopub.execute_input": "2025-02-22T15:11:49.726331Z",
     "iopub.status.idle": "2025-02-22T15:11:49.732004Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.726302Z",
     "shell.execute_reply": "2025-02-22T15:11:49.730391Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.293688Z",
     "start_time": "2025-02-23T22:49:00.291695Z"
    }
   },
   "outputs": [],
   "execution_count": 127
  },
  {
   "cell_type": "markdown",
   "source": "# Attentions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Vanilla",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class VanillaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                 .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
    "                                                               is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = nn.functional.softmax(att, dim=-1)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y "
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.733557Z",
     "iopub.execute_input": "2025-02-22T15:11:49.734058Z",
     "iopub.status.idle": "2025-02-22T15:11:49.761872Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.734017Z",
     "shell.execute_reply": "2025-02-22T15:11:49.759956Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.321303Z",
     "start_time": "2025-02-23T22:49:00.316341Z"
    }
   },
   "outputs": [],
   "execution_count": 128
  },
  {
   "cell_type": "markdown",
   "source": "## Nystrom",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class NystromAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Linformer self-attention mechanism with linear complexity.\n",
    "    Projects keys and values to a lower dimensional space for efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_landmarks = config.attention_config.get('nystrom_landmarks', 32) if config.attention_config else 32\n",
    "        print(f'Nystrom with landmarks {self.n_landmarks}')\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "        # mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        hs = C // self.n_head\n",
    "\n",
    "        # calculate query, key, values for all heads in batch\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, hs).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        \n",
    "        params = {'B': B, 'nh': self.n_head, 'T': T, 'hs': hs}\n",
    "        \n",
    "        # Project keys and values to lower dimensional space\n",
    "        q_landmarks = self.__get_landmark_representation(q, self.n_landmarks, **params)\n",
    "        k_landmarks = self.__get_landmark_representation(k, self.n_landmarks, **params)\n",
    "        \n",
    "        # Compute the attention matrix\n",
    "        L = F.softmax(q @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1)\n",
    "        P = self.__iterative_inv(F.softmax(q_landmarks @ k_landmarks.transpose(-1, -2) / math.sqrt(hs), dim=-1))\n",
    "\n",
    "        N_prod = (q_landmarks @ k.transpose(-1, -2))\n",
    "        # print(N_prod.shape)\n",
    "        # print(self.bias.shape)\n",
    "        # print(q_landmarks.shape)\n",
    "        N_masked = N_prod.masked_fill(self.bias[:, :, :self.n_landmarks, :T] == 0, float('-inf'))\n",
    "        N = F.softmax(N_masked / math.sqrt(hs), dim=-1)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        att = L @ P @ N\n",
    "\n",
    "        # Apply attention to values and reshape\n",
    "        y = att @ v  # (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        return self.c_proj(y)\n",
    "    \n",
    "    def __get_landmark_representation(self, tensor, num_landmarks, B, nh, T, hs):\n",
    "        tensor_reshaped = tensor.reshape(-1, nh, num_landmarks, T // num_landmarks, hs) # (B, nh, T, hs)\n",
    "        tensor_landmarks = tensor_reshaped.mean(dim=-2)\n",
    "        return tensor_landmarks\n",
    "\n",
    "    def __iterative_inv(self, mat, n_iter=6):\n",
    "        I = torch.eye(mat.size(-1), device=mat.device)\n",
    "        K = mat\n",
    "\n",
    "        # The entries of K are positive and ||K||_{\\infty} = 1 due to softmax\n",
    "        V = 1 / torch.max(torch.sum(K, dim=-2), dim = -1).values[:, :, None, None] * K.transpose(-1, -2)\n",
    "\n",
    "        for _ in range(n_iter):\n",
    "            KV = torch.matmul(K, V)\n",
    "            V = torch.matmul(0.25 * V, 13 * I - torch.matmul(KV, 15 * I - torch.matmul(KV, 7 * I - KV)))\n",
    "        return V"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:18:45.393591Z",
     "iopub.execute_input": "2025-02-22T15:18:45.393946Z",
     "iopub.status.idle": "2025-02-22T15:18:45.409608Z",
     "shell.execute_reply.started": "2025-02-22T15:18:45.393917Z",
     "shell.execute_reply": "2025-02-22T15:18:45.408276Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.337239Z",
     "start_time": "2025-02-23T22:49:00.330474Z"
    }
   },
   "outputs": [],
   "execution_count": 129
  },
  {
   "cell_type": "markdown",
   "source": "## Linformer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "def gen_causal_mask(input_size, dim_k, full_attention=False):\n",
    "    \"\"\"\n",
    "    Generates a causal mask of size (input_size, dim_k) for Linformer.\n",
    "    When full_attention is True, returns an (input_size, input_size) mask.\n",
    "    \"\"\"\n",
    "    if full_attention:\n",
    "        return (torch.triu(torch.ones(input_size, input_size)) == 1).transpose(0, 1)\n",
    "    return (torch.triu(torch.ones(dim_k, input_size)) == 1).transpose(0, 1)\n",
    "\n",
    "class LinformerAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Ensure the embedding is divisible by number of heads\n",
    "        assert config.n_embd % config.n_head == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        # linformer_k: reduced sequence length (e.g., 64) for keys/values projections\n",
    "        self.linformer_k = config.block_size // 4\n",
    "        self.causal = getattr(config, \"causal\", True)\n",
    "\n",
    "        # Single linear layer to produce queries, keys, and values (as in Nanogpt)\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        # Learnable projection matrices for keys and values:\n",
    "        # They map from the original sequence length (block_size) to the lower dimension (linformer_k)\n",
    "        self.E = nn.Parameter(torch.randn(self.block_size, self.linformer_k))\n",
    "        self.F = nn.Parameter(torch.randn(self.block_size, self.linformer_k))\n",
    "\n",
    "        # Attempt to initialize E and F from pickle files if available.\n",
    "        with open(\"../out-shakespeare-char/linformer_E_256.pkl\", \"rb\") as f:\n",
    "            E_init = pickle.load(f)[\"block_1\"]\n",
    "            self.E = nn.Parameter(E_init)\n",
    "\n",
    "        with open(\"../out-shakespeare-char/linformer_F_256.pkl\", \"rb\") as f:\n",
    "            F_init = pickle.load(f)[\"block_1\"]\n",
    "            self.F = nn.Parameter(F_init)\n",
    "\n",
    "        # Precompute causal mask if needed (mask shape: (block_size, linformer_k))\n",
    "        if self.causal:\n",
    "            self.register_buffer(\"causal_mask\", gen_causal_mask(self.block_size, self.linformer_k, full_attention=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Compute queries, keys, values in one go and split them up\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        # Reshape and transpose to get shape (B, n_head, T, head_dim)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply Linformer projection: project along the sequence dimension.\n",
    "        # k_proj: (B, n_head, linformer_k, head_dim)\n",
    "        # v_proj: (B, n_head, linformer_k, head_dim)\n",
    "        k_proj = torch.einsum('bhtd,tk->bhkd', k, self.E)\n",
    "        v_proj = torch.einsum('bhtd,tk->bhkd', v, self.F)\n",
    "\n",
    "        # Compute attention scores using the projected keys.\n",
    "        # q: (B, n_head, T, head_dim)\n",
    "        # k_proj.transpose(-2, -1): (B, n_head, head_dim, linformer_k)\n",
    "        att = (q @ k_proj.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if self.causal:\n",
    "            # Use only the first T rows of the precomputed causal mask.\n",
    "            # causal_mask shape: (T, linformer_k)\n",
    "            mask = self.causal_mask[:T, :]\n",
    "            att = att.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        att = att.softmax(dim=-1)\n",
    "\n",
    "        # Multiply attention weights with projected values.\n",
    "        # Resulting shape: (B, n_head, T, head_dim)\n",
    "        y = att @ v_proj\n",
    "\n",
    "        # Reassemble multi-head outputs.\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.827839Z",
     "iopub.execute_input": "2025-02-22T15:11:49.828295Z",
     "iopub.status.idle": "2025-02-22T15:11:49.839855Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.828260Z",
     "shell.execute_reply": "2025-02-22T15:11:49.838611Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.345332Z",
     "start_time": "2025-02-23T22:49:00.339698Z"
    }
   },
   "outputs": [],
   "execution_count": 130
  },
  {
   "cell_type": "markdown",
   "source": "## Performer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class CausalPerformerAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.n_features = config.attention_config.get('performer_features', 64) \\\n",
    "            if config.attention_config else 64\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=True)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=True)\n",
    "\n",
    "        # random projection for performer\n",
    "        proj = torch.randn(self.head_size, self.n_features) * 0.1\n",
    "        self.register_buffer(\"proj\", proj)\n",
    "\n",
    "        # causal mask for safety (though the prefix-sum approach is also doing it)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # random feature mapping\n",
    "        k_prime = self._prime(k)  # (B, n_head, T, n_features)\n",
    "        q_prime = self._prime(q)\n",
    "\n",
    "        # prefix sums along T\n",
    "        kprime_v = k_prime.unsqueeze(-1) * v.unsqueeze(-2)   # (B, n_head, T, n_features, head_size)\n",
    "        prefix_k = torch.cumsum(k_prime, dim=2)              # (B, n_head, T, n_features)\n",
    "        prefix_kv = torch.cumsum(kprime_v, dim=2)            # (B, n_head, T, n_features, head_size)\n",
    "\n",
    "        # numerator: q_prime[t, :] dot prefix_kv[t, :, :]\n",
    "        numerator = torch.einsum('b n t f, b n t f d -> b n t d', q_prime, prefix_kv)\n",
    "        # denominator\n",
    "        denominator = torch.einsum('b n t f, b n t f -> b n t', q_prime, prefix_k) + 1e-6\n",
    "\n",
    "        out = numerator / denominator.unsqueeze(-1)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(out)\n",
    "\n",
    "    def _prime(self, x):\n",
    "        # phi(x) = exp(xW - ||x||^2/2) / sqrt(n_features)\n",
    "        norm_sq = torch.sum(x**2, dim=-1, keepdim=True)\n",
    "        x_proj = torch.einsum('b n t d, d f -> b n t f', x, self.proj)\n",
    "        x_exp = torch.exp(x_proj - 0.5 * norm_sq)\n",
    "        return x_exp * (1.0 / math.sqrt(self.n_features))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.841193Z",
     "iopub.execute_input": "2025-02-22T15:11:49.841497Z",
     "iopub.status.idle": "2025-02-22T15:11:49.863809Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.841472Z",
     "shell.execute_reply": "2025-02-22T15:11:49.862678Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.358492Z",
     "start_time": "2025-02-23T22:49:00.353648Z"
    }
   },
   "outputs": [],
   "execution_count": 131
  },
  {
   "cell_type": "markdown",
   "source": "# Config",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class GPTConfig:\n",
    "    block_size: int = 4096\n",
    "    vocab_size: int = 65  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    attention_config = {'nystrom_landmarks': 64}\n",
    "    bias = 0.2"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:21:31.145958Z",
     "iopub.execute_input": "2025-02-22T15:21:31.146278Z",
     "iopub.status.idle": "2025-02-22T15:21:31.151653Z",
     "shell.execute_reply.started": "2025-02-22T15:21:31.146253Z",
     "shell.execute_reply": "2025-02-22T15:21:31.150132Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.371953Z",
     "start_time": "2025-02-23T22:49:00.370262Z"
    }
   },
   "outputs": [],
   "execution_count": 132
  },
  {
   "cell_type": "markdown",
   "source": "# Util",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def prepare_tokens(x, config, wte, wpe, ln):\n    # In: token embeddings of shape (1, t)\n    b, t = x.size()\n    pos = torch.arange(0, t, dtype=torch.long, device='cpu').unsqueeze(0)  # shape (1, t)\n    tok_emb = wte(x)  # token embeddings of shape (b, t, n_embd)\n    pos_emb = wpe(pos)  # position embeddings of shape (1, t, n_embd)\n    coded_x = tok_emb + pos_emb\n    norm_x = ln(coded_x)\n    return norm_x # token embeddings of shape (1, t, n_embd)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.894022Z",
     "iopub.execute_input": "2025-02-22T15:11:49.894412Z",
     "iopub.status.idle": "2025-02-22T15:11:49.914979Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.894362Z",
     "shell.execute_reply": "2025-02-22T15:11:49.913707Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.375590Z",
     "start_time": "2025-02-23T22:49:00.373798Z"
    }
   },
   "outputs": [],
   "execution_count": 133
  },
  {
   "cell_type": "code",
   "source": "def get_config(block_size):\n    config = GPTConfig()\n    config.block_size = block_size\n\n    wte=nn.Embedding(config.vocab_size, config.n_embd)\n    wpe=nn.Embedding(config.block_size, config.n_embd)\n    ln = nn.LayerNorm(config.n_embd, bias=False)\n\n    return config, wte, wpe, ln",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.916239Z",
     "iopub.execute_input": "2025-02-22T15:11:49.916785Z",
     "iopub.status.idle": "2025-02-22T15:11:49.937849Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.916745Z",
     "shell.execute_reply": "2025-02-22T15:11:49.936273Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.384007Z",
     "start_time": "2025-02-23T22:49:00.382068Z"
    }
   },
   "outputs": [],
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "source": "def get_tokens(x_np_array, config, wte, wpe, ln):\n    print(f'Tokens shape (batch_size, context_window): {x_np_array.shape}')\n    x = torch.stack([torch.from_numpy(tokens) for tokens in x_np_array])\n    return prepare_tokens(x, config, wte, wpe, ln)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.939827Z",
     "iopub.execute_input": "2025-02-22T15:11:49.940182Z",
     "iopub.status.idle": "2025-02-22T15:11:49.962070Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.940146Z",
     "shell.execute_reply": "2025-02-22T15:11:49.960852Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.391799Z",
     "start_time": "2025-02-23T22:49:00.389926Z"
    }
   },
   "outputs": [],
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "source": "with torch.no_grad():\n    def get_attention_matrix(att_type, block_size, x):\n        torch.manual_seed(42)\n        np.random.seed(42)\n        config, wte, wpe, ln = get_config(block_size)\n    \n        if att_type == 'linformer':\n            attention = LinformerAttention(config)\n        elif att_type == 'nystrom':\n            attention = NystromAttention(config)\n        elif att_type == 'vanilla':\n            attention = VanillaAttention(config)\n        elif att_type == 'performer':\n            attention = CausalPerformerAttention(config)\n    \n        tokens = get_tokens(x[:1, :block_size], config, wte, wpe, ln)\n        att_matrix = attention.forward(tokens)\n        print(f'obtained attention matrix of shape {att_matrix.shape}')\n        att_matrix = att_matrix\n            \n        return att_matrix",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:19:28.667390Z",
     "iopub.execute_input": "2025-02-22T15:19:28.667794Z",
     "iopub.status.idle": "2025-02-22T15:19:28.674094Z",
     "shell.execute_reply.started": "2025-02-22T15:19:28.667763Z",
     "shell.execute_reply": "2025-02-22T15:19:28.672956Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.400528Z",
     "start_time": "2025-02-23T22:49:00.398005Z"
    }
   },
   "outputs": [],
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "source": "# Metrics",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:49.991965Z",
     "iopub.execute_input": "2025-02-22T15:11:49.992319Z",
     "iopub.status.idle": "2025-02-22T15:11:50.013101Z",
     "shell.execute_reply.started": "2025-02-22T15:11:49.992287Z",
     "shell.execute_reply": "2025-02-22T15:11:50.012101Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.408271Z",
     "start_time": "2025-02-23T22:49:00.406797Z"
    }
   },
   "outputs": [],
   "execution_count": 137
  },
  {
   "cell_type": "code",
   "source": [
    "x_array = np.loadtxt('./shakespeare_char/tokens10k.txt', delimiter=',')\n",
    "x_np = np.array([x_array]).astype(np.int64)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:50.014220Z",
     "iopub.execute_input": "2025-02-22T15:11:50.014483Z",
     "iopub.status.idle": "2025-02-22T15:11:50.046210Z",
     "shell.execute_reply.started": "2025-02-22T15:11:50.014460Z",
     "shell.execute_reply": "2025-02-22T15:11:50.044823Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.416971Z",
     "start_time": "2025-02-23T22:49:00.414528Z"
    }
   },
   "outputs": [],
   "execution_count": 138
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "attention_type = 'vanilla'\n",
    "block_size = 256\n",
    "attention_matrix = get_attention_matrix(attention_type, block_size, x_np)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:16:03.381384Z",
     "iopub.execute_input": "2025-02-22T15:16:03.381761Z",
     "iopub.status.idle": "2025-02-22T15:16:03.406380Z",
     "shell.execute_reply.started": "2025-02-22T15:16:03.381731Z",
     "shell.execute_reply": "2025-02-22T15:16:03.404942Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.430660Z",
     "start_time": "2025-02-23T22:49:00.422983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape (batch_size, context_window): (1, 256)\n",
      "obtained attention matrix of shape torch.Size([1, 256, 384])\n",
      "CPU times: user 10.6 ms, sys: 4.58 ms, total: 15.1 ms\n",
      "Wall time: 5.7 ms\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "source": "config, wte, wpe, ln = get_config(block_size)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:11:50.132738Z",
     "iopub.execute_input": "2025-02-22T15:11:50.133137Z",
     "iopub.status.idle": "2025-02-22T15:11:50.140201Z",
     "shell.execute_reply.started": "2025-02-22T15:11:50.133086Z",
     "shell.execute_reply": "2025-02-22T15:11:50.138981Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.441561Z",
     "start_time": "2025-02-23T22:49:00.438164Z"
    }
   },
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-22T15:22:28.379149Z",
     "iopub.execute_input": "2025-02-22T15:22:28.379467Z",
     "iopub.status.idle": "2025-02-22T15:22:38.116604Z",
     "shell.execute_reply.started": "2025-02-22T15:22:28.379441Z",
     "shell.execute_reply": "2025-02-22T15:22:38.115499Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:49:00.521709Z",
     "start_time": "2025-02-23T22:49:00.455549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vanilla_out = get_attention_matrix('vanilla', block_size, x_np).clone().detach()\n",
    "\n",
    "# compare with Nystrom\n",
    "nystrom_out = get_attention_matrix('nystrom', block_size, x_np).clone().detach()\n",
    "sim_nys = nn.functional.cosine_similarity(vanilla_out.view(-1), nystrom_out.view(-1), dim=0)\n",
    "print(f\"Vanilla vs Nystrom similarity: {sim_nys.item():.4f}\")\n",
    "\n",
    "# compare with Linformer\n",
    "linformer_out = get_attention_matrix('linformer', block_size, x_np).clone().detach()\n",
    "sim_lin = nn.functional.cosine_similarity(vanilla_out.view(-1), linformer_out.view(-1), dim=0)\n",
    "print(f\"Vanilla vs Linformer similarity: {sim_lin.item():.4f}\")\n",
    "\n",
    "# compare with Performer\n",
    "performer_out = get_attention_matrix('performer', block_size, x_np).clone().detach()\n",
    "sim_per = nn.functional.cosine_similarity(vanilla_out.view(-1), performer_out.view(-1), dim=0)\n",
    "print(f\"Vanilla vs Performer similarity: {sim_per.item():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape (batch_size, context_window): (1, 256)\n",
      "obtained attention matrix of shape torch.Size([1, 256, 384])\n",
      "Nystrom with landmarks 64\n",
      "Tokens shape (batch_size, context_window): (1, 256)\n",
      "obtained attention matrix of shape torch.Size([1, 256, 384])\n",
      "Vanilla vs Nystrom similarity: 0.7824\n",
      "Tokens shape (batch_size, context_window): (1, 256)\n",
      "obtained attention matrix of shape torch.Size([1, 256, 384])\n",
      "Vanilla vs Linformer similarity: 0.0281\n",
      "Tokens shape (batch_size, context_window): (1, 256)\n",
      "obtained attention matrix of shape torch.Size([1, 256, 384])\n",
      "Vanilla vs Performer similarity: 0.5644\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  }
 ]
}
